##简单说明
*Enron* 数据集共有517439个文档 <br>
*newsgroups* 数据集共有19998个文档 <br>
我们使用*Enron*数据集中大于1KB的文档进行实验，共365665个，
因为小于1KB的文档中只含有很少的有用信息，提取到的关键字非常
有限。
####函数说明
**ClassifyFiles**  
根据文件大小进行分类，方便后续选择具有足够词汇量的文档<br>
**test**<br>
关键字提取流程：<br>
使用Python的NLTK对文章中的单词进行归类分析
后，提取单词的词干。同时剔除单词长度小于4或大于10的单词，并且
剔除含有数字的单词。<br>
对文件进行预处理后，我们使用sklearn包的TfidfVectorizer模块
根据词频提取文档中的关键字。最终我们提取得到XX个关键字，共XX对
文档-关键字。<br>
`max_df=0.85, min_df=0.05, max_features=1000`提取关键字
的参数
####设置说明